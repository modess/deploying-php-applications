# 9. Running tests

We can (almost) all agree on that tests are good for software and they should be written to some extent. How large your test suite is and how specific it is depends on your application and your teams culture of writing tests. *There are no rules* when it comes to testing, only subjective ideas and thoughts. Some advocate for 100% code coverage and this can come from trying to force developers to write tests, or that someone is a neat freak, or it can be that someone just finds it a nice and even number. I would say that 100% code coverage is rarely a good approach since it provides a lot of overhead both for your test suite and also in developer time. Finding the sweet-spot in your test suite is usually not a percentage number but instead of finding the right code to test and have good tests in place for it. Setters and getters are usually unnecessary pieces of code to test for example, but a class that deals with credit card payments should be thoroughly tested.

This will not be a chapter on how, why and when to write tests, there are plenty of resources out there that are extensive and written by people with far better testing experience and knowledge. Instead this will be a chapter on the different types of tests and how they can or should fit into your deploy process. There are so many types of tests you can write that it would be impossible for me to cover all of them and instead I will cover the ones most commonly used in a PHP context. Testing is under constant debate without a clearly defined terminology with an array of terms being thrown around. I will not try to define types of tests but instead give a general term for them and explain the thought behind them since it's good to understand them so you can leverage that instead of a label. Unfortunately I must use a label when writing about them.

What I do want to get across is that tests should always be a part of your deploy process. Tests are perfect for automation and the full test suite should at some point run before code ends up in your production environment.

## 9.1 Unit testing

The most basic of tests are unit tests and I do not believe there is another term used for these types of tests. A *unit* can be defined as a small, as small as possible in fact, piece of code that can be isolated and subjected to an automated test. They are fast, sharp and precise like a frickin' laser beam. Also no tests should depend on the state of another test but each one should run on a clean slate. The most common tool used for unit testing in the PHP world is PHPUnit and I doubt there will be a successor to it soon.

Unit tests are all about finding bugs as soon as possible. It's a kind of first line defense against errors in your code and with a [TDD](https://en.wikipedia.org/wiki/Test-driven_development) workflow, where you write tests first, you can often reduce the amount of possible bugs down the road. Write a test, watch it fail, write code, check if code passes through tests, rinse and repeat. The TDD world is not without opponents of course and there was a lot of talk going on in the developer community when David Heinemeier Hansson release his blog post on ["TDD is dead. Long live testing."](http://david.heinemeierhansson.com/2014/tdd-is-dead-long-live-testing.html) back in 2014. Take a look at both camps and see what you find.

## 9.2 Acceptance testing

In the agile software world we work with specifications and use cases. These will often be transformed into a specification of behavior, or business logic that a certain feature should comply with. Acceptance testing is used if you use behavior driven development (BDD) with tools such as [Behat](http://behat.org/), [PHPSpec](http://www.phpspec.net/), etc. In some cases these tests will use Selenium or Mink to run automated tests against a browser which makes it questionable if it's an acceptance test at that point, in other cases these tests will be quite fast to run. I argue that in cases of automated tests against a browser it's an end-to-end test.

When performing an acceptance test you're trying to answer the question if the feature was built correctly according to a specification. A test could run against a user story that is converted to the Cucumber language, such as this user registration feature:

```
Feature: User registration

  Scenario: Registration form submission
    Given I am a guest user
    And I enter the email “foo@bar.com”
    And I enter the password “abc123”
    When I submit the registration form
    Then I should have a user registered with the email “foo@bar.com”
```

This type of testing is great since it’s a very readable format that can be shared between developers and non-developers so they can agree on the specification before the developer implements it. It removes language barriers and translation layers, and allows for better communication. If you’re more interested in BDD I suggest you take a look at the talk [Taking back BDD](https://skillsmatter.com/skillscasts/6240-taking-back-bdd) by Konstantin Kudryashov.

## 9.3 End-to-end testing

These automated tests run against an emulated browser which often makes them slow to run, but they can ensure software quality against an entire system (such as a web page). They could perhaps make sure users are able to get through the entire process of registering, logging in or making a credit card payment for example. The test runner needs to start a browser and click through the process while waiting for the browser like a regular user which can be very time consuming.

A common tool used for this in PHP is [Mink](http://mink.behat.org/) which has support for a various number of drivers that can emulate web browsers. The different drivers have different modes of operation and in that way have different speed as well. Some are just headless browsers that are quite fast but lack support for Javascript, would you want to test a registration form that sends an XHR-request for example you’re out of luck with them. But then there are some that can start full feature web browsers such as Chrome, Firefox or Internet Explorer, but these are a lot slower since a browser needs to be started and the tests will have to use it as a regular user would.

## 9.4 Manual testing

Unless your doing continuous deployment you will do manual testing somewhere, whether it is only in your local environment by yourself or by a quality assurance (QA) team/person. You pretty much sit like a monkey and click on and type in various things to make sure it doesn’t break, you’re probably trying your best to make it break. That was maybe harsh on QA-people, it takes skill and experience to perform good manual tests and trying to find the edge cases where a feature can break.

You should try to apply *hallway usability testing* whenever possible which is defined by Joel Spolsky in the classic [The Joel Test: 12 Steps to Better Code](http://www.joelonsoftware.com/articles/fog0000000043.html) as:

> A hallway usability test is where you grab the next person that passes by in the hallway and force them to try to use the code you just wrote. If you do this to five people, you will learn 95% of what there is to learn about usability problems in your code.

With this technique you will find a lot of usability issues and bugs you can fix before deploying. Do this as early as possible in your chain of environments to make sure you catch stuff you then have time to fix. Doing this on a complete release branch pushed to staging could end up delaying the deploy or lead to a *we’ll fix it later* issue.

## 9.5 What, where and when

To fit the tests into your deploy process is determining what should run where and when. If you have a large application with lots of tests that takes a long time to run, running your test suite on each commit is probably going to annoy the hell out of every developer. You want your test suite to be as fast as possible of course, but some tests such as end-to-end tests will probably be slow. You need to find a good balance here, ultimately we want to find any errors before our code gets deployed in production.

In a best case scenario you have a continuous integration environment, such as TravisCI, Jenkins or CodeShip to name a few. It can be responsible of picking up changes and running through your entire test suite once you push to a repository. If you do not have this luxury I would recommend running any time consuming tests as early as possible just after a developers local environment. Developers should be able to run unit and acceptance tests in their local environment without any issue since they should be fast. But other than that it’s good to run them in your development or staging environment once a developer pushes changes to the main repository. Having a set up where running the entire test suite in your development environment that pushes all the changes to your staging environment on a successful test run is to strive for. Most important is to **automate running tests**.

## 9.6 Failing tests

So how would you deal with a failing test? Ignoring failing, incomplete or skipped tests will lead to code rot in your test suite and I advice you to never let this happen. In accordance to the broken window theory it will likely end up with no new tests being written, old tests not getting fixed properly and eventually you will have to scrap and rewrite your test suite. Going back to Joel Spolsky’s [The Joel Test: 12 Steps to Better Code](http://www.joelonsoftware.com/articles/fog0000000043.html), number five on the list is *Do you fix bugs before writing new code?*. This one can not really be applied here but I suggest a new bullet to the list: *Do you fix tests before making a deploy to production?*.

It’s important to **stop** your deploy process once a test fails, make sure the right people gets **notified** and that it’s managed as a prioritized task to fix the code or the test to make it pass. A deploy with any kind of failing tests should never be allowed to complete in your production environment. This is true even if you know why a test is failing and know that nothing is broken. Fix it straight away and start the deploy process again. It will promote a good culture and a good test suite for your application that is not subject to rot.